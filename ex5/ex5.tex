\documentclass[10pt]{article}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[fleqn]{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage[ruled,linesnumbered,noend]{algorithm2e}
\usepackage{empheq}
\usepackage{float}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage[colorlinks=true,urlcolor=blue]{hyperref}

\title{Introduction to Machine Learning, Fall 2014 - Exercise session V}
\author{Rodion ``rodde'' Efremov \\ 013593012}

\begin{document}
 \maketitle

\color{blue}
\section*{Problem 1 (3 points)}
Consider the following two sets of 80 cars each: Set `A' consists of 10 Volvos, 25 Toyotas, and 45 Audis, while set `B' consists of 8 Volvos, 32 Toyotas, and 40 Audis. Which set do you intuitively think is more pure (that is, has lower impurity), and why? Compute the entropy, the Gini index, and the misclassification error for each of the two sets. According to these measures, which set is more pure? Could this phenomenon (conflict among the measures) happen if there were just two classes (two types of car) rather than three? Why, or why not?

\color{black}
To me, it seems that the set `A' is more pure than `B', for ``intervals'' in set `A' are more ``equal'' than in the set `B'.

The entropy of the set A is ($\log = \log_2$)
\begin{align*}
&-\frac{10}{80} \log \frac{10}{80} - \frac{25}{80} \log \frac{25}{80} - \frac{45}{80} \log \frac{45}{80} \\ 
&= \frac{1}{8}(\log 80 - \log 10) +\frac{5}{16} (\log 80 - \log 25) + \frac{9}{16} (\log 80 - \log 45) \\
 &\approx 0.375 + 0.524 + 0.467 \\
 &\approx 1.366.
\end{align*}

The entropy of the set B is
\begin{align*}
&-\frac{8}{80} \log \frac{8}{80} - \frac{32}{80} \log \frac{32}{80} - \frac{40}{80} \log \frac{40}{80} \\
&= \frac{1}{10} \log 10 + \frac{4}{10} (\log 80 - \log 32) + \frac{1}{2} \log 2 \\
&\approx 0.332+ 0.529 + 0.5 \\
&\approx 1.361.
\end{align*}

The Gini index of the set A is
\begin{align*}
&1 - \Bigg( \frac{10}{80} \Bigg)^2 - \Bigg( \frac{25}{80} \Bigg)^2 - \Bigg( \frac{45}{80}  \Bigg)^2 \\
&= 1 - (1 / 8)^2 - (5/16)^2 - (9/16)^2 \\
&\approx 1 - 0.016 - 0.098 - 0.316 \\
&\approx 0.570.
\end{align*}

The Gini index of the set B is
\begin{align*}
&1 - \Bigg( \frac{8}{80} \Bigg)^2 - \Bigg( \frac{32}{80} \Bigg)^2 - \Bigg( \frac{40}{80} \Bigg)^2 \\
&\approx 1 - 0.01 - 0.16 - 0.25 \\
&\approx 0.58.
\end{align*}

The classification error of the set A is
\begin{align*}
1 - \frac{45}{80} = 1 - \frac{9}{16} = 0.4375.
\end{align*}

The classification error of the set B is
\begin{align*}
1 - \frac{40}{80} = 0.5.
\end{align*}

In summary:
\begin{table}[h]
\centering
\begin{tabular}[H]{|c||c|c|c|}
\hline
 & Entropy & Gini index & Classification error \\
 \hline
 A & 1.366 & 0.570 & 0.438 \\
 \hline
 B & 1.361 & 0.580 & 0.5 \\
 \hline
\end{tabular}
\end{table}
According to the above table, the set A is slightly more pure than B.

Now, let us restrict the amount of car types only to two. (Say, Lada and Rolls Royce.) Let $N$ be the total amount of cars, and let $L$ denote the amount of Lada's (so that we have $N - L$ RR's). Now the entropy of such a set is
\begin{align*}
& -\frac{L}{N} \log \frac{L}{N} - \frac{N - L}{N} \log \frac{N - L}{N} = \\
& \frac{L}{N}\log \frac{N}{L} + \Bigg( 1 - \frac{L}{N} \Bigg) \log \frac{N}{N - L} = \\
& \frac{L}{N} \log N - \frac{L}{N} \log L + \Bigg( 1 - \frac{L}{N} \Bigg) \Bigg( \log N - \log (N - L) \Bigg) = \\
& -\frac{L}{N} \log L + \log N - \log (N - L) + \frac{L}{N} \log(N - L) = \\
&a
\end{align*}

\color{blue}
\section*{Problem 2 (3 points)}

\color{blue}
\section*{Problem 3 (3 points)}

\color{blue}
\section*{Problem 4 (15 points)}

\end{document}