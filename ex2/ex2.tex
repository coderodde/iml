\documentclass[10pt]{article}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage[ruled,linesnumbered,noend]{algorithm2e}
\usepackage{empheq}
\usepackage{float}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage[colorlinks=true,urlcolor=blue]{hyperref}

\title{Introduction to Machine Learning, Fall 2014 - Exercise session II}
\author{Rodion ``rodde'' Efremov}

\begin{document}
 \maketitle

\color{blue}
\section*{Problem 1 (3 points)}
Consider a document-term matrix, where $tf_{ij}$ is the number of times that the $i^{th}$ word (term) appeares in the $j^{th}$ document, and let $m$ be the total number of documents in the collection. Consider the variable transformation that is defined by
\begin{align}
tf'_{ij} = tf_{ij} \log \frac{m}{df_i},
\end{align}
where $df_i$ is the number of documents in which the $i^{th}$ term appears, which is known as the \textit{document frequency} of the term. This transformation is known as the \textit{inverse document frequency} transformation.
\begin{itemize}
\item[(a)] What is the effect of this transformation if a term occurs in only one document? In every document?
\item[(b)] What is the overall effect and what might be the purpose of this transformation?
\item[(c)] Can you think of other (non-document) data in which this transformation might be useful?
\end{itemize}

\color{black}
\subsection*{(a)}
If the the term occurs in only one document, we have $df_i = 1$, and, thus, $tf'_{ij} = tf_{ij}\log m$. If $df_i = m$, we have that $tf'_{ij} = tf_{ij}\log \frac{m}{m} = 0$.

\subsection*{(b)}
The inverse document frequency aims to minimize the effect of over-emphasized terms such as ``the'', ``in'', and so on, and to emphasize the words that are not used very often, and, thus, allow better chance of differentiating between documents.

\subsection*{(c)}
Suppose that the rows of the matrix represent particular market baskets of individual shoppers, and the columns represent particular items. Now, the ''\textit{inverse item frequency}'' lessens the effect of the items that tend to be ''popular'', such as bread or milk, for instance.

\color{blue}
\section*{Problem 2 (3 points)}
In this exercise we explore the relationships between the cosine and correlation similarity measures and Euclidean distance for data vectors in $\mathbb{R}^n$.

\begin{itemize}
\item[(a)] What is the range of values that are possible for the cosine measure?
\item[(b)] If two objects have a cosine measure of 1, are they necessarily identical? Explain.
\item[(c)] What is the relationship of the cosine measure to correlation, if any? (Hint: Look at statistical measures such as mean and standard deviation in cases where cosine and correlation are the same and different.)
\item[(d)] Derive the mathematical relationship between cosine similarity and Euclidean distance when each data object has an $L_2$ length (norm) of 1.
\item[(e)] Derive the mathematical relationship between correlation and Euclidean distance when each data point has been standardized by substracting its mean and dividing by its standard deviation.
\end{itemize}

\color{black}
\subsection*{(a)}
The range of values is the range of values of cosine function, i.e., $[-1, 1]$.

\subsection*{(b)}
If the cosine measure of two objects $\textbf{x}$ and $\textbf{y}$ is 1, it implies that the two vectors have zero degree angle, i.e. they have the same orientation. Because the cosine measure is agnostic to the lengths of the vectors however, it implies that $\textbf{x} = a\textbf{y}$ for some real $a > 0$.

\subsection*{(c)}
Suppose we are given two data vectors $X, Y$, and their respective means are $\bar{X}, \bar{Y}$.
Now we have that the correlation coefficient of the two is 
\[
r_{X, Y} = \frac{\sum_{k = 1}^n \big( X_k - \bar{X} \big) \big( Y_k - \bar{Y} \big)}{ \sqrt{ \sum_{ k = 1 }^n \big(X_k - \bar{X} \big)^2 } \sqrt{ \sum_{k = 1}^n \big(Y_k - \bar{Y} \big)^2 } }.
\]
If $A = (A_1, \dots, A_n) \in \mathbb{R}^n$ and $\alpha \in \mathbb{R}$, by $A + \alpha$ we denote the vector $(A_1 + \alpha, \dots, A_n + \alpha)$. Now, let us substitute $X' = X - \bar{X}$ and $Y' = Y - \bar{Y}$. We obtain
\begin{align*}
r_{X, Y} &= \frac{\sum_{k = 1}^n \big( X_k - \bar{X} \big) \big( Y_k - \bar{Y} \big)}{ \sqrt{ \sum_{ k = 1 }^n \big(X_k - \bar{X} \big)^2 } \sqrt{ \sum_{k = 1}^n \big(Y_k - \bar{Y} \big)^2 } } \\
            &= \frac{ \sum_{k = 1}^n X'_k Y'_k }{ \sqrt{ \sum_{k = 1}^n {X'}_k^2 }  \sqrt{ \sum_{k = 1}^n {Y'}_{k}^2} } \\
            &= \frac{ X' \cdot Y' }{ |X'| |Y'| } \\
            &= \cos(X', Y') \\
            &= \cos(X - \bar{X}, Y - \bar{Y}).
\end{align*}
 
\subsection*{(d)}
Suppose we are given two data objects $X$ and $Y$, both having an $L_2$ length of 1. Now we have that
\begin{align*}
d(X, Y) &= \sqrt{\sum_{k = 1}^n \bigg( X_k - Y_k \bigg)^2} \\
           &= \sqrt{\sum_{k = 1}^n \bigg( X_k^2 + Y_k^2 - 2X_kY_k \bigg)} \\
           &= \sqrt{|X|^2 + |Y|^2 - 2 \sum_{k = 1}^n X_kY_k} \\
           &= \sqrt{|X|^2 + |Y|^2 - 2 \frac{X \cdot Y}{|X||Y|}} \\
           &= \sqrt{|X|^2 + |Y|^2 - 2 \cos \theta_{X,Y}} \\
           &= \sqrt{2 - 2 \cos \theta_{X,Y}} \\
           &= \sqrt{2(1 - \cos \theta_{X,Y})} \in [0, 2].        
\end{align*}
where $\cos \theta_{X,Y}$ is the cosine measure of the vectors $X$ and $Y$.

\subsection*{(e)}
Suppose we are given two $n$-vectors $X'$ and $Y'$. We compute the means of both of them, and denote the aforementioned as $\bar{X'}$ and $\bar{Y'}$. Next, we compute standard deviations of both, $\sigma_{X'}$ and $\sigma_{Y'}$, after which we have all the data needed to standardize $X'$ and $Y'$: for each component, we subtract the respective mean and divide by the respective standard deviation, and thus, we obtain two new vectors $X$ and $Y$ with both having means $\bar{X} = \bar{Y} = 0$ and standard deviation $\sigma_X = \sigma_Y = 1$.  Now we have that
\begin{align*}
d^2(X, Y) &= \sum_{k = 1}^n (X_k - Y_k)^2 \\
                 &= \sum_{k = 1}^n (X_k^2 + Y_k^2 - 2X_k Y_k) \\
                 &= |X|^2 + |Y|^2 - 2(n - 1) \Bigg( \frac{1}{n - 1} \sum_{k = 1}^n X_k Y_k \Bigg) \\
                 &= |X|^2 + |Y|^2 - 2(n - 1) \Bigg( \frac{\frac{1}{n - 1} \sum_{k = 1}^n (X_k - \bar{X})(Y_k - \bar{Y})}{\sigma_X \sigma_Y}\Bigg) \\
                 &= |X|^2 + |Y|^2 - 2(n - 1)r_{X, Y},
\end{align*}
where $r_{X, Y}$ denotes the correlation factor of the vectors $X, Y$.

% problem 3 %

\color{blue}
\section*{Problem 3 (3 points)}
Proximity is typically defined between a pair of objects.
\begin{itemize}
\item[(a)] Give two ways in which you might define the 'proximity' among a set of (more than two) objects (i.e. a single measure of how similar an arbitrary number of items are all to one another)
\item[(b)] How might you define the distance between two sets of points in Euclidian space?
\item[(c)] How might you define the proximity between two sets of data objects? (Make no assumptions about the data objects, except that a proximity measure is defined between any pair of objects.)
\end{itemize}

\color{black}
\subsection*{(a)}
If $P(\textbf{x}, \textbf{y})$ is a ``normal'' function giving the proximity of data objects $\textbf{x}$ and $\textbf{y}$, and $A$ is an arbitrary set of data objects (which can be indexed like $A_1, A_2, \dots,\\ A_n$), I would try 
\[
P(A) = \Bigg( \sum_{1 \leq i < j \leq n} P(A_i, A_j) \Bigg)\binom{n}{2}^{-1},
\]
or namely the average proximity over all pairs of data objects from $A$. For another one, I would choose a small $\varepsilon > 0$, and define $P(A)$ as
\[
\prod_{1 \leq i < j \leq n} \bigg( \max \big( P(A_i, A_j), \varepsilon \big) \bigg)^{ \frac{1}{\binom{n}{2}}} \geq \varepsilon,
\]
or namely the geometric mean over all pairs of data objects from $A$. The $\varepsilon$ is choosed to be positive so that a single proximity value of 0 does not dominate the entire proximity of a set and bring it down to 0. (Above, it is assumed that $P(\textbf{x}, \textbf{y}) \geq 0$ for all data objects $\textbf{x}, \textbf{y}$.)
 
\subsection*{(b)}
 I would do the way topologists do: if $A$ and $B$ are two (finite) sets of points in Euclidian space, then
 \[
 P(A, B) = \underset{(\textbf{x}, \textbf{y}) \in A \times B}{\min} d(\textbf{x}, \textbf{y}).
 \]
 
\subsection*{(c)}
Suppose we are given two sets of data objects, $A$ and $B$. Now, the easiest way to define proximity between them is
\[
P(A, B) = \sum_{\textbf{x} \in A} \sum_{\textbf{y} \in B} \frac{P(\textbf{x}, \textbf{y})}{|A||B|}.
\]  

\color{blue}
\section*{Problem 4 (15 points)}
In this problem we will consider similarity measures for movies on the Movielens dataset.
 \color{black}
 
\begin{itemize}
 \item[(a)] 
 \begin{verbatim}
 # Octave/Matlab
 cd path/to/movielens
 run('loadmovielens.m')
 [ratings items userids itemids] = loadmovielens();
 \end{verbatim}
 
 \item[(b)]
 The function below loads a map mapping each movie ID $i$ to a list of user ID's of those users, that have ranked the film $i$.
 \begin{verbatim}
 function m = load_movie_to_rankings_map(items, ratings)
 m = {};
 # Initialize the map keys (movie IDs).
 for mid = 1:size(items)(2)
   m{mid} = [];
 endfor
 # For each ranking...
 for rid = 1:size(ratings)(1)
   r = ratings(rid,:); 
   m{r(2)} (size(m{r(2)}) + 1) = r(1);   
 endfor
 endfunction
 \end{verbatim}  

 The actual Jaccard measure:
 \begin{verbatim}
 function coef = Jaccard(id1, id2, map)
   a = map{id1};
   b = map{id2};
   coef = size(intersect(a, b))(2) / size(union(a, b))(2);
 endfunction
 \end{verbatim}

 \begin{verbatim}
 function coef = Jaccard2(id1, id2, ratings)
   coef = -1;
   a = [];
   b = [];
   # Accumulate sets of user IDs:
   # 'a' for the users who ranked the film with ID 'id1', and
   # 'b' for the users who ranked the film with ID 'id2'.
   for i = 1:size(ratings)(1)
     if (ratings(i, 2) == id1)
       # Really nasty looking "idiom" for appending to a list.
       a(size(a)(2) + 1) = ratings(i, 1);
     elseif (ratings(i, 2) == id2)
       b(size(b)(2) + 1) = ratings(i, 1);
     endif 
   endfor
   coef = size(intersect(a,b))(2) / size(union(a, b))(2);
 endfunction
 \end{verbatim}
 The ID of 'Three Colors: Red' is 59, and the ID of 'Tree Colors: Blue' is 60, and the above function returns a Jaccard coefficient of 0.59783 for them.
 \end{itemize}

Okay, its near 7 PM Friday, so I have to give up on this one. :'(  Jaccard(id1, id2, map) gives me consistently 0.21548 instead of 0.21712 for Toy Story vs. GoldenEye.

\end{document}