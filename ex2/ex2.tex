\documentclass[10pt]{article}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage[ruled,linesnumbered,noend]{algorithm2e}
\usepackage{empheq}
\usepackage{float}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage[colorlinks=true,urlcolor=blue]{hyperref}

\title{Introduction to Machine Learning, Fall 2014 - Exercise session II}
\author{Rodion ``rodde'' Efremov}

\begin{document}
 \maketitle

\color{red}
\section*{Problem 1 (3 points)}
Consider a document-term matrix, where $tf_{ij}$ is the number of times that the $i^{th}$ word (term) appeares in the $j^{th}$ document, and let $m$ be the total number of documents in the collection. Consider the variable transformation that is defined by
\begin{align}
tf'_{ij} = tf_{ij} \log \frac{m}{df_i},
\end{align}
where $df_i$ is the number of documents in which the $i^{th}$ term appears, which is known as the \textit{document frequency} of the term. This transformation is known as the \textit{inverse document frequency} transformation.
\begin{itemize}
\item[(a)] What is the effect of this transformation if a term occurs in only one document? In every document?
\item[(b)] What is the overall effect and what might be the purpose of this transformation?
\item[(c)] Can you think of other (non-document) data in which this transformation might be useful?
\end{itemize}

\color{black}
\subsection*{(a)}
If the the term occurs in only one document, we have $df_i = 1$, and, thus, $tf'_{ij} = tf_{ij}\log m$. If $df_i = m$, we have that $tf'_{ij} = tf_{ij}\log \frac{m}{m} = 0$.

\subsection*{(b)}
The inverse document frequency aims to minimize the effect of over-emphasized terms such as ``the'', ``in'', and so on, and to emphasize the words that are not used very often, and, thus, allow better chance of differentiating between documents.

\subsection*{(c)}
Suppose that the rows of the matrix represent particular market baskets of individual shoppers, and the columns represent particular items. Now, the ''\textit{inverse item frequency}'' lessens the effect of the items that tend to be ''popular'', such as bread or milk, for instance.

\color{red}
\section*{Problem 2 (3 points)}
In this exercise we explore the relationships between the cosine and correlation similarity measures and Euclidean distance for data vectors in $\mathbb{R}^n$.

\begin{itemize}
\item[(a)] What is the range of values that are possible for the cosine measure?
\item[(b)] If two objects have a cosine measure of 1, are they necessarily identical? Explain.
\item[(c)] What is the relationship of the cosine measure to correlation, if any? (Hint: Look at statistical measures such as mean and standard deviation in cases where cosine and correlation are the same and different.)
\item[(d)] Derive the mathematical relationship between cosine similarity and Euclidean distance when each data object has an $L_2$ length (norm) of 1.
\item[(e)] Derive the mathematical relationship between correlation and Euclidean distance when each data point has been standardized by substracting its mean and dividing by its standard deviation.
\end{itemize}

\color{black}
\subsection*{(a)}
The range of values is the range of values of cosine function, i.e., $[-1, 1]$.

\subsection*{(b)}
If the cosine measure of two objects $\textbf{x}$ and $\textbf{y}$ is 1, it implies that the two vectors have zero degree angle, i.e. they have the same orientation. Because the cosine measure is agnostic to the lengths of the vectors however, it implies that $\textbf{x} = a\textbf{y}$ for some real $a > 0$.

\subsection*{(c)}

\subsection*{(d)}
Suppose we are given two data objects $X$ and $Y$, both having an $L_2$ length of 1. Now we have that
\begin{align*}
d(X, Y) &= \sqrt{\sum_{k = 1}^n \bigg( X_k - Y_k \bigg)^2} \\
           &= \sqrt{\sum_{k = 1}^n \bigg( X_k^2 + Y_k^2 - 2X_kY_k \bigg)} \\
           &= \sqrt{|X|^2 + |Y|^2 - 2 \sum_{k = 1}^n X_kY_k} \\
           &= \sqrt{|X|^2 + |Y|^2 - 2 \frac{X \cdot Y}{|X||Y|}} \\
           &= \sqrt{|X|^2 + |Y|^2 - 2 \cos \theta_{X,Y}},           
\end{align*}
where $\cos \theta_{X,Y}$ is the cosine measure of the vectors $X$ and $Y$.

\subsection*{(e)}
Suppose we are given two $n$-vectors $X'$ and $Y'$. We compute the means of both of them, and denote the aforementioned as $\bar{X'}$ and $\bar{Y'}$. Next, we compute standard deviations of both, $\sigma_{X'}$ and $\sigma_{Y'}$, after which we have all the data needed to standardize $X'$ and $Y'$: for each component, we subtract the respective mean and divide by the respective standard deviation, and thus, we obtain two new vectors $X$ and $Y$ with both having means $\bar{X} = \bar{Y} = 0$ and standard deviation $\sigma_X = \sigma_Y = 1$.  Now we have that
\begin{align*}
d^2(X, Y) &= \sum_{k = 1}^n (X_k - Y_k)^2 \\
                 &= \sum_{k = 1}^n (X_k^2 + Y_k^2 - 2X_k Y_k) \\
                 &= |X|^2 + |Y|^2 - 2n \Bigg( \frac{1}{n} \sum_{k = 1}^n X_k Y_k \Bigg) \\
                 &= |X|^2 + |Y|^2 - 2n \Bigg( \frac{\frac{1}{n} \sum_{k = 1}^n (X_k - \bar{X})(Y_k - \bar{Y})}{\sigma_X \sigma_Y}\Bigg) \\
                 &= |X|^2 + |Y|^2 - 2nr,
\end{align*}
where $r$ denotes the correlation factor of the vectors $X, Y$.
\color{red}
\section*{Problem 3 (3 points)}
Proximity is typically defined between a pair of objects.
\begin{itemize}
\item[(a)] Give two ways in which you might define the 'proximity' among a set of (more than two) objects (i.e. a single measure of how similar an arbitrary number of items are all to one another)
\item[(b)] How might you define the distance between two sets of points in Euclidian space?
\item[(c)] How might you define the proximity between two sets of data objects? (Make no assumptions about the data objects, except that a proximity measure is defined between any pair of objects.)
\end{itemize}

\color{black}
\subsection*{(a)}
If $P(\textbf{x}, \textbf{y})$ is a ``normal'' function giving the proximity of data objects $\textbf{x}$ and $\textbf{y}$, and $A$ is an arbitrary set of data objects (which can be indexed like $A_1, A_2, \dots,\\ A_n$), I would try 
\[
P(A) = \Bigg( \sum_{1 \leq i < j \leq n} P(A_i, A_j) \Bigg)\binom{n}{2}^{-1},
\]
or namely the average proximity over all pairs of data objects from $A$. For another one, I would choose a small $\varepsilon > 0$, and define $P(A)$ as
\[
\prod_{1 \leq i < j \leq n} \bigg( \max \big( P(A_i, A_j), \varepsilon \big) \bigg)^{ \frac{1}{\binom{n}{2}}} \geq \varepsilon,
\]
or namely the geometric mean over all pairs of data objects from $A$. The $\varepsilon$ is choosed to be positive so that a single proximity value of 0 does not dominate the entire proximity of a set and bring it down to 0. (Above, it is assumed that $P(\textbf{x}, \textbf{y}) \geq 0$ for all data objects $\textbf{x}, \textbf{y}$.)
 
\subsection*{(b)}
 I would do the way topologists do: if $A$ and $B$ are two (finite) sets of points in Euclidian space, then
 \[
 P(A, B) = \underset{(\textbf{x}, \textbf{y}) \in A \times B}{\min} d(\textbf{x}, \textbf{y}).
 \]
 
\subsection*{(c)}
Suppose we are given two sets of data objects, $A$ and $B$. Now, the easiest way to define proximity between them is
\[
P(A, B) = \sum_{\textbf{x} \in A} \sum_{\textbf{y} \in B} \frac{P(\textbf{x}, \textbf{y})}{|A||B|}.
\]  
\end{document}