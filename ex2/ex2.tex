\documentclass[10pt]{article}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage[ruled,linesnumbered,noend]{algorithm2e}
\usepackage{empheq}
\usepackage{float}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage[colorlinks=true,urlcolor=blue]{hyperref}

\title{Introduction to Machine Learning, Fall 2014 - Exercise session II}
\author{Rodion ``rodde'' Efremov}

\begin{document}
 \maketitle

\color{red}
\section*{Problem 1 (3 points)}
Consider a document-term matrix, where $tf_{ij}$ is the number of times that the $i^{th}$ word (term) appeares in the $j^{th}$ document, and let $m$ be the total number of documents in the collection. Consider the variable transformation that is defined by
\begin{align}
tf'_{ij} = tf_{ij} \log \frac{m}{df_i},
\end{align}
where $df_i$ is the number of documents in which the $i^{th}$ term appears, which is known as the \textit{document frequency} of the term. This transformation is known as the \textit{inverse document frequency} transformation.
\begin{itemize}
\item[(a)] What is the effect of this transformation if a term occurs in only one document? In every document?
\item[(b)] What is the overall effect and what might be the purpose of this transformation?
\item[(c)] Can you think of other (non-document) data in which this transformation might be useful?
\end{itemize}

\color{black}
\subsection*{(a)}
If the the term occurs in only one document, we have $df_i = 1$, and, thus, $tf'_{ij} = tf_{ij}\log m$. If $df_i = m$, we have that $tf'_{ij} = tf_{ij}\log \frac{m}{m} = 0$.

\subsection*{(b)}
The inverse document frequency aims to minimize the effect of over-emphasized terms such as ``the'', ``in'', and so on, and to emphasize the words that are not used very often, and, thus, allow better chance of differentiating between documents.

\color{red}
\section*{Problem 3 (3 points)}
Proximity is typically defined between a pair of objects.
\begin{itemize}
\item[(a)] Give two ways in which you might define the 'proximity' among a set of (more than two) objects (i.e. a single measure of how similar an arbitrary number of items are all to one another)
\item[(b)] How might you define the distance between two sets of points in Euclidian space?
\item[(c)] How might you define the proximity between two sets of data objects? (Make no assumptions about the data objects, except that a proximity measure is defined between any pair of objects.)
\end{itemize}

\color{black}
\subsection*{(a)}
If $P(\textbf{x}, \textbf{y})$ is a ``normal'' function giving the proximity of data objects $\textbf{x}$ and $\textbf{y}$, and $A$ is an arbitrary set of data objects (which can be indexed like $A_1, A_2, \dots,\\ A_n$), I would try 
\[
P(A) = \Bigg( \sum_{1 \leq i < j \leq n} P(A_i, A_j) \Bigg)\binom{n}{2}^{-1},
\]
or namely the average proximity over all pairs of data objects from $A$. For another one, I would choose a small $\varepsilon > 0$, and define $P(A)$ as
\[
\prod_{1 \leq i < j \leq n} \bigg( \max \big( P(A_i, A_j), \varepsilon \big) \bigg)^{ \frac{1}{\binom{n}{2}}} \geq \varepsilon,
\]
or namely the geometric mean over all pairs of data objects from $A$. The $\varepsilon$ is choosed to be positive so that a single proximity value of 0 does not dominate the entire proximity of a set and bring it down to 0. (Above, it is assumed that $P(\textbf{x}, \textbf{y}) \geq 0$ for all data objects $\textbf{x}, \textbf{y}$.)
 
\subsection*{(b)}
 I would do the way topologists do: if $A$ and $B$ are two (finite) sets of points in Euclidian space, then
 \[
 P(A, B) = \underset{(\textbf{x}, \textbf{y}) \in A \times B}{\min} d(\textbf{x}, \textbf{y}).
 \]
 
\subsection*{(c)}
Suppose we are given two sets of data objects, $A$ and $B$. Now, the easiest way to define proximity between them is
\[
P(A, B) = \sum_{\textbf{x} \in A} \sum_{\textbf{y} \in B} \frac{P(\textbf{x}, \textbf{y})}{|A||B|}.
\]  
\end{document}